{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All of the imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim import fully_connected as fc\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our data. Just load in the MNIST dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "num_sample = mnist.train.num_examples\n",
    "# Flattened input 28*28=784\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VariationAutoencoder(object):\n",
    "    def __init__(self, learning_rate=1e-4, batch_size=100, n_z=5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_z = n_z\n",
    "\n",
    "        self.build()\n",
    "        \n",
    "        # Launch the session.\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        self.x = tf.placeholder(name='x', dtype=tf.float32, shape=(None,\n",
    "            input_dim))\n",
    "        \n",
    "        # Encoder\n",
    "        # slim.fc(input, output_dim, scope, activation function)\n",
    "        f1 = fc(self.x, 512, scope='enc_fullc1', activation_fn = tf.nn.elu)\n",
    "        f2 = fc(f1, 384, scope='enc_fullc2', activation_fn = tf.nn.elu)\n",
    "        f3 = fc(f2, 256, scope='enc_fullc3', activation_fn = tf.nn.elu)\n",
    "        \n",
    "        # Output dimension should be the latent dimension.\n",
    "        self.z_mu = fc(f3, self.n_z, scope='enc_fc4_mu', activation_fn=None)\n",
    "        # log(sigma^2)\n",
    "        self.log_sigma_z_sq = fc(f3, self.n_z, scope='enc_fc4_sigma_sq', activation_fn=None)\n",
    "        \n",
    "        # N(z_mu, z_sigma)\n",
    "        # Generate z from the normal distribution\n",
    "        eps = tf.random_normal(shape=tf.shape(self.log_sigma_z_sq), mean=0, stddev=1, dtype=tf.float32)\n",
    "        self.z = self.z_mu + tf.sqrt(tf.exp(self.log_sigma_z_sq)) * eps\n",
    "        \n",
    "        # Decoder\n",
    "        d1 = fc(self.z, 256, scope='dec_fc1', activation_fn=tf.nn.elu)\n",
    "        d2 = fc(self.z, 384, scope='dec_fc2', activation_fn=tf.nn.elu)\n",
    "        d3 = fc(self.z, 512, scope='dec_fc3', activation_fn=tf.nn.elu)\n",
    "        self.x_hat = fc(d3, input_dim, scope='dec_fc4', activation_fn=tf.sigmoid)\n",
    "        \n",
    "        # Losses\n",
    "        # reconstruction loss between x and x hat \n",
    "        \n",
    "        # H(x, x_hat) = - \\Sigma x * log(x_hat) + (1-x) * log(1 - x_hat)\n",
    "        epsilon = 1e-10\n",
    "        recon_loss = -tf.reduce_sum(\n",
    "            self.x * tf.log(self.x_hat + epsilon) + (1 - self.x) * tf.log(1 - self.x_hat + epsilon),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # latent distribution loss\n",
    "        # Use the kale divergence to measure the difference between two distributions\n",
    "        # The latent distribution and N(0, 1)\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + self.log_sigma_z_sq - tf.square(self.z_mu) - tf.exp(self.log_sigma_z_sq),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # total loss (that just combines the two losses together)\n",
    "        self.total_loss = tf.reduce_mean(recon_loss + latent_loss)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate).minimize(self.total_loss)\n",
    "        \n",
    "        \n",
    "    # Execute a forward and a backward pass\n",
    "    # report the loss for monitoring\n",
    "    def run_single_step(self, x):\n",
    "        _, loss = self.sess.run([self.train_op, self.total_loss], feed_dict={self.x: x})\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    # Reconstruction\n",
    "    # x -> x_hat\n",
    "    def reconstructor(self, x):\n",
    "        return self.sess.run(self.x_hat, feed_dict={self.x: x})\n",
    "    \n",
    "    \n",
    "    # Generation\n",
    "    def generator(self, z):\n",
    "        return self.sess_run(self.x_hat, feed_dict={self.z: z})\n",
    "    \n",
    "    \n",
    "    # Transformer\n",
    "    # x -> z\n",
    "    def transformer(self, x):\n",
    "        return self.sess.run(self.z, feed_dict={self.x: x})\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainer(learning_rate = 1e-4, batch_size = 100, num_epochs = 100, n_z = 10):\n",
    "    # Create the model\n",
    "    model = VariationAutoencoder(learning_rate = learning_rate, \n",
    "                                   batch_size = batch_size, n_z = n_z)\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_i in range(num_sample // batch_size):\n",
    "            # Obtain a mini-batch \n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # train network, execute a forward and backward pass\n",
    "            # We only care about the image not the label.\n",
    "            loss = model.run_single_step(batch[0])\n",
    "            \n",
    "        print('[Epoch %i] Loss: %.6f' % (epoch, loss))\n",
    "        \n",
    "    print('Done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = trainer(learning_rate = 1e-4, batch_size = 100, num_epochs = 5, n_z = 5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
